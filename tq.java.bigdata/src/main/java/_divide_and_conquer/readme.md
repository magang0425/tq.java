2019-04-08

## 分治法解决大数据查询问题

### 海量日志数据，提取出某日访问次数最多的那个IP
1. 先映射, 而后统计, 最后排序
2. 解法   
    - hash映射
        - 从日志中将IP提提取, 逐个写入大文件中, 采用映射的方法, %1000, 映射为 1000个小文件
    - hash_map统计
        - 采用 hash_map(ip, value) 分别对1000个小文件中的IP进行统计, 找出文件中出现频率最大的那个
    - 堆/快速排序
        - 根据各自的出现频率进行排序(堆排序), 找出频率最大的IP
        
### 寻找热门查询，300万个查询字符串中统计最热门的10个查询(TOP K)
1. hash_map + 堆
2. 解法
    - hash_map统计 
        - 维护一个Key为Query字串，Value为该Query出现次数的hash_map，即hash_map(Query, Value)，
        - 每次读取一个Query，如果该字串不在Table中，那么加入该字串，并将Value值设为1；
        - 如果该字串在Table中，那么将该字串的计数加1 即可。最终我们在O(N)的时间复杂度内用hash_map完成了统计；
    - 堆排序
        - 找出Top K，时间复杂度为N‘logK。即借助堆结构，我们可以在log量级的时间内查找和调整/移动。
        - 因此，维护一个K(该题目中是10)大小的小根堆，然后遍历300万的Query，分别和根元素进行对比。
        - 所以，我们最终的时间复杂度是：O(n) + N' * O(logk），其中，N为1000万，N’为300万
    - 最小堆

### 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词
1. 分而治之/hash映射 
    - 顺序读取文件，对于每个词x，取hash(x)%5000，然后把该值存到5000个小文件（记为x0,x1,...x4999）中。
    - 这样每个文件大概是200k左右。
    - 当然，如果其中有的小文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。
2. hash_map统计 
    - 对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。
3. 堆/归并排序 
    - 取出出现频率最大的100个词（可以用含100个结点的最小堆）后，
    - 再把100个词及相应的频率存入文件，这样又得到了5000个文件。
    - 最后就是把这5000个文件进行归并（类似于归并排序）的过程了
    

### 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10
1. 解法一
    - 如果同一个数据元素只出现在某一台机器中: 堆排序(最大, 最小), 组合归并
2. 解法二
    - 如果同一个元素重复出现在不同的电脑中呢: hash取模 + 解法一

### 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序
1. 解法一
    - hash 映射 + hash_map统计 + 堆/快速/归并排序
2. 解法二
    - 如果重复率比较高, 可以直接采用 hash_map
3. 解法三
    - 做完hash，分成多个文件后，可以交给多个文件来处理 + MapReduce, 最后合并

### 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？
1. 解法一
    - 分而治之/hash映射
        - 遍历文件a, hash(url) % 1000, 1000个小文件(a0, a1, a2, .... , a999), b同样(b0, b1, b2, ... b999)
        - 这样处理后，所有可能相同的url都在对应的小文件(a0 vs b0, a1 vs b1, ... a999 vs b999)
    - hash_set 统计
        - 求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。
        - 然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了

### 100万个数中找出最大的100个数
1. 局部淘汰法
    - 选取前100个元素，并排序，记为序列L
    - 插入排序
    - O(100w*100)
2. 最小堆
    - 含100个元素的最小堆完成
    - O(100w * lg100)
3. 快速排序
    - TODO
    
### 举一反三
1. 怎么在海量数据中找出重复次数最多的一个？
    - hash, 取模, 求出每个小文件重复次数最多的, 记录重复次数, 然后找出最多的
2. 上千万或上亿数据（有重复），统计其中出现次数最多的前N个数据。
    - ash_map/搜索二叉树/红黑树等来进行统计次数。
    - 然后就是取出前N个出现次数最多的数据了，堆机制完成。
3. 一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析
    - TODO
4. 1000万字符串，其中有些是重复的，需要把重复的全部去掉，保留没有重复的字符串。请怎么设计和实现？
    - 这题用trie树比较合适，hash_map也行。当然，也可以先hash成小文件分开处理再综合。
5. 一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，问最优解。
    - 首先根据用hash并求模，将文件分解为多个小文件，
    - 对于单个文件利用上题的方法求出每个文件件中10个最常出现的词。
    - 然后再进行归并处理，找出最终的10个最常出现的词。